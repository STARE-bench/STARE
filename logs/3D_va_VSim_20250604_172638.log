phoenix-srun: job 5092596 queued and waiting for resources
phoenix-srun: job 5092596 has been allocated resources
phoenix-srun: Job 5092596 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

[root] Loading dataset /mnt/petrelfs/gujiawei/stare_bench/release_stare/stare.jsonl, category: ['3D_va_VSim']
[root] Loading local JSONL file: /mnt/petrelfs/gujiawei/stare_bench/release_stare/stare.jsonl
[root] Loading config
[root] Loading local model /mnt/petrelfs/share_data/songmingyang/model/mm/Qwen2-VL-2B-Instruct
[2025-06-04 17:27:01,664] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[qwen_vl_utils.vision_process] set VIDEO_TOTAL_PIXELS: 90316800
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[root] Model loaded!
[root] Starting to generate.....
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:14<48:09, 14.23s/it]  1%|          | 2/204 [00:19<30:22,  9.02s/it][root] Save results to /mnt/petrelfs/gujiawei/stare_bench/release_stare/stare-bench/results/stare_3D_va_VSim_direct.json
  1%|▏         | 3/204 [00:24<23:58,  7.16s/it]  2%|▏         | 4/204 [00:29<20:28,  6.14s/it]  2%|▏         | 5/204 [00:34<19:35,  5.91s/it]  3%|▎         | 6/204 [00:39<18:21,  5.57s/it]  3%|▎         | 7/204 [00:44<17:30,  5.33s/it]  4%|▍         | 8/204 [00:48<16:25,  5.03s/it]  4%|▍         | 9/204 [00:53<16:29,  5.07s/it]  5%|▍         | 10/204 [00:58<16:02,  4.96s/it][root] Save results to /mnt/petrelfs/gujiawei/stare_bench/release_stare/stare-bench/results/stare_3D_va_VSim_direct.json
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 56.85 GiB is free. Including non-PyTorch memory, this process has 22.48 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 209.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  5%|▌         | 11/204 [01:03<15:56,  4.95s/it]  6%|▌         | 12/204 [01:07<15:12,  4.75s/it]  6%|▋         | 13/204 [01:12<15:17,  4.80s/it]  7%|▋         | 14/204 [01:17<15:11,  4.80s/it]  7%|▋         | 15/204 [01:22<15:10,  4.82s/it]  8%|▊         | 16/204 [01:27<15:16,  4.87s/it]  8%|▊         | 17/204 [01:32<15:23,  4.94s/it]  9%|▉         | 18/204 [01:37<15:05,  4.87s/it]  9%|▉         | 19/204 [01:41<14:33,  4.72s/it] 10%|▉         | 20/204 [01:46<14:38,  4.77s/it][root] Save results to /mnt/petrelfs/gujiawei/stare_bench/release_stare/stare-bench/results/stare_3D_va_VSim_direct.json
 10%|█         | 21/204 [01:51<14:25,  4.73s/it] 11%|█         | 22/204 [01:56<14:44,  4.86s/it] 11%|█▏        | 23/204 [02:01<14:37,  4.85s/it] 12%|█▏        | 24/204 [02:06<14:38,  4.88s/it] 12%|█▏        | 25/204 [02:10<14:22,  4.82s/it] 13%|█▎        | 26/204 [02:15<14:28,  4.88s/it] 13%|█▎        | 27/204 [02:20<14:25,  4.89s/it] 14%|█▎        | 28/204 [02:25<14:34,  4.97s/it] 14%|█▍        | 29/204 [02:30<14:20,  4.92s/it] 15%|█▍        | 30/204 [02:35<14:18,  4.93s/it][root] Save results to /mnt/petrelfs/gujiawei/stare_bench/release_stare/stare-bench/results/stare_3D_va_VSim_direct.json
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
phoenix-srun: interrupt (one more within 1 sec to abort)
phoenix-srun: StepId=5092596.0 task 0: running
phoenix-srun: sending Ctrl-C to StepId=5092596.0
phoenix-srun: Job step aborted: Waiting up to 2 seconds for job step to finish.
phoenix-srun: Easily find out why your job was killed by following the link below:
	https://docs.phoenix.sensetime.com/FAQ/SlurmFAQ/Find-out-why-my-job-was-killed/
 15%|█▌        | 31/204 [02:40<14:00,  4.86s/it] 16%|█▌        | 32/204 [02:45<14:13,  4.96s/it] 16%|█▌        | 33/204 [02:50<14:11,  4.98s/it] 17%|█▋        | 34/204 [02:55<13:44,  4.85s/it] 17%|█▋        | 35/204 [03:00<13:45,  4.88s/it] 18%|█▊        | 36/204 [03:05<14:05,  5.03s/it] 18%|█▊        | 37/204 [03:10<13:49,  4.97s/it] 19%|█▊        | 38/204 [03:15<13:41,  4.95s/it] 19%|█▉        | 39/204 [03:20<14:07,  5.14s/it] 20%|█▉        | 40/204 [03:25<13:58,  5.11s/it]slurmstepd: error: *** STEP 5092596.0 ON SH-IDCA1404-10-140-54-22 CANCELLED AT 2025-06-04T17:30:52 ***
 20%|█▉        | 40/204 [03:30<14:21,  5.25s/it]
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 196.95 GiB. GPU 0 has a total capacty of 79.32 GiB of which 44.93 GiB is free. Including non-PyTorch memory, this process has 34.40 GiB memory in use. Of the allocated memory 20.63 GiB is allocated by PyTorch, and 12.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/mnt/petrelfs/gujiawei/stare_bench/release_stare/stare-bench/generate_response.py", line 180, in <module>
    main()
  File "/mnt/petrelfs/gujiawei/stare_bench/release_stare/stare-bench/generate_response.py", line 131, in main
phoenix-srun: forcing job termination
phoenix-srun: launch/slurm: _step_signal: Terminating StepId=5092596.0
